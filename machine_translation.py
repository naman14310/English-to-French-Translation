# -*- coding: utf-8 -*-
"""machine_translation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DJ2wROZ6njy5JGCuCt_NV-vHkSMULOsF
"""

import numpy as np 
import re
import string
import nltk
from nltk.stem import WordNetLemmatizer
!pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git &> /dev/null
from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer
from sklearn.model_selection import train_test_split
from keras.layers import Input, LSTM, Embedding, Dense
from keras.models import Model
from nltk.translate.bleu_score import sentence_bleu

from google.colab import drive
drive.mount('/content/drive')

TRAIN_BLEU_PATH = "/content/drive/MyDrive/NLP_A3_dataset/2020201080_MT1_train.txt"
TEST_BLEU_PATH = "/content/drive/MyDrive/NLP_A3_dataset/2020201080_MT1_test.txt"

eng_train_path = "/content/drive/MyDrive/NLP_A3_dataset/ted-talks-corpus/train.en"
eng_train = open(eng_train_path).readlines()

eng_test_path = "/content/drive/MyDrive/NLP_A3_dataset/ted-talks-corpus/test.en"
eng_test = open(eng_test_path).readlines()

eng_dev_path = "/content/drive/MyDrive/NLP_A3_dataset/ted-talks-corpus/dev.en"
eng_dev = open(eng_dev_path).readlines()

fr_train_path = "/content/drive/MyDrive/NLP_A3_dataset/ted-talks-corpus/train.fr"
fr_train = open(fr_train_path).readlines()

fr_test_path = "/content/drive/MyDrive/NLP_A3_dataset/ted-talks-corpus/test.fr"
fr_test = open(fr_test_path).readlines()

fr_dev_path = "/content/drive/MyDrive/NLP_A3_dataset/ted-talks-corpus/dev.fr"
fr_dev = open(fr_dev_path).readlines()

"""## Data Cleaning

### English Dataset
"""

def preprocess_data(dataset):

    data = []

    for sent in dataset:

        sent = sent.replace("'",'').lower()
        sent = re.sub('[^A-Za-z]+', ' ', sent).strip()

        sent = nltk.tokenize.word_tokenize(sent)
        sent = [i for i in sent if i not in nltk.corpus.stopwords.words('english')]
        sent = (WordNetLemmatizer().lemmatize(word) for word in sent)

        data.append(sent)

    return data

"""### French Dataset"""

def preprocess_french_data(dataset):

    data = []

    for sent in dataset[:1000]:

        sent = sent.replace("'",'').lower()
        sent = re.sub('[^A-Za-z]+', ' ', sent).strip()        

        sent = nltk.tokenize.word_tokenize(sent)
        sent = [i for i in sent if i not in nltk.corpus.stopwords.words('french')]
        sent = (FrenchLefffLemmatizer().lemmatize(w) for w in sent)

        sent = "START_ " + sent + " _END"
        data.append(sent)

    return data

eng_train = preprocess_data(eng_train)
eng_dev = preprocess_data(eng_dev)
eng_test = preprocess_data(eng_test)

# print("For English text\n")

# print("Training data length : ", len(eng_train))
# print("Testing data length : ", len(eng_test))
# print("Dev data length : ", len(eng_dev))

# print("\n--------------------------------------------\n")

fr_train = preprocess_french_data(fr_train)
fr_dev = preprocess_french_data(fr_dev)
fr_test = preprocess_french_data(fr_test)

# print("For French text\n")

# print("Training data length : ", len(fr_train))
# print("Testing data length : ", len(fr_test))
# print("Dev data length : ", len(fr_dev))

eng_train[:5]

fr_train[:5]

"""### Creating Vocab"""

def create_vocab(data):
    vocab = set()

    for sent in data:
        for w in sent.split():
            vocab.add(w)

    return vocab


en_vocab = create_vocab(eng_train)
encoder_tkns = len(en_vocab)
print("Total english words : ", len(en_vocab))

fr_vocab = create_vocab(fr_train)
decoder_tkns = len(fr_vocab) +1
print("Total french words : ", len(fr_vocab))

input_words = sorted(list(en_vocab))
target_words = sorted(list(fr_vocab))

def find_maxLen(data):
    l = []

    for sent in data:
        w = sent.split()
        l.append(len(w))
    
    return max(l)

maxLen_src = find_maxLen(eng_train)
print(maxLen_src)

maxLen_tar = find_maxLen(fr_train)
print(maxLen_tar)

def get_tkn_idx(words):
    return dict([(w, i+1) for i, w in enumerate(words)])

def get_reverse_idx(tkn_idx):
    return dict((i, w) for w, i in tkn_idx.items())


src_tkn_idx = get_tkn_idx(input_words)
reverse_src_idx = get_reverse_idx(src_tkn_idx)

tar_tkn_idx = get_tkn_idx(target_words)
reverse_tar_idx = get_reverse_idx(tar_tkn_idx)

x_train, x_test, y_train, y_test = train_test_split(eng_train, fr_train, test_size = 0.2, random_state = 55)

# print(len(x_train))
# print(len(x_test))
# print(len(y_train))
# print(len(y_test))

BATCH_SIZE = 16

def create_zero_layer(size):
    return np.zeros ( (BATCH_SIZE, size), dtype = 'float32')

def create_batch(x, y):

    while 1:

        lx = len(x)
        for j in range(0, lx, BATCH_SIZE):

            encoder_ip = create_zero_layer(maxLen_src)
            
            decoder_ip = create_zero_layer(maxLen_tar)

            decoder_tar = np.zeros((BATCH_SIZE, maxLen_tar, decoder_tkns), dtype = 'float32')

            start = j
            end = j + BATCH_SIZE

            for i, (src, tar) in enumerate( zip(x[start : end], y[start : end]) ):

                for t, w in enumerate(src.split()):
                    idx = src_tkn_idx[w]
                    encoder_ip[i, t] = idx

                for t, w in enumerate(tar.split()):

                    tar_idx = tar_tkn_idx[w]

                    if t > 0:
                        t_1 = t-1
                        decoder_tar[i, t_1, tar_idx] = 1

                    tsplit = len(tar.split())-1

                    if t < tsplit:
                        decoder_ip[i, t] = tar_idx

                    else:
                        pass

                yield([encoder_ip, decoder_ip], decoder_tar)

"""### Encoder Decoder Architecture

#### Encoder
"""

LATENT_SIZE = 150

def get_input_layer():
    return Input(shape = (None, ))

encoder_ip = get_input_layer()

encoder_lstm = LSTM(LATENT_SIZE, return_state = True)
encoder_op, val1, val2 = encoder_lstm(Embedding(encoder_tkns, LATENT_SIZE, mask_zero = True)(encoder_ip))

"""#### Decoder"""

decoder_ip = get_input_layer()
dec_embedding = Embedding(decoder_tkns, LATENT_SIZE, mask_zero = True)

decoder_lstm = LSTM(LATENT_SIZE, return_sequences = True, return_state = True)
decoder_op, _, _ = decoder_lstm(dec_embedding(decoder_ip), initial_state = [val1, val2])

decoder_dense = Dense(decoder_tkns, activation = 'softmax')

model = Model ([encoder_ip, decoder_ip], decoder_dense(decoder_op))

model.compile(optimizer = 'adam', loss = 'categorical_crossentropy')
model.summary()

STEPS_PER_EPOCH = len(x_train) // BATCH_SIZE
SREPS_PER_EPOCH_validation = len(x_test) // BATCH_SIZE

model.fit_generator(generator = create_batch(x_train, y_train), steps_per_epoch = STEPS_PER_EPOCH, epochs = 50, validation_data = create_batch(x_test, y_test), validation_steps = SREPS_PER_EPOCH_validation )

model.save('/content/drive/MyDrive/NLP_A3_dataset/seq2seq_model.h5')

"""### Loading Pretrained Model"""

model.load('/content/drive/MyDrive/NLP_A3_dataset/seq2seq_model.h5')

def get_decoder_states():
    return [Input(shape = (LATENT_SIZE, )), Input(shape = (LATENT_SIZE, ))]

def get_decoder_lstm_layer(decoder_states_ip):
    return decoder_lstm(dec_embedding(decoder_ip), initial_state = decoder_states_ip)

encoder_states = [val1, val2]

encoder_model = Model(encoder_ip, encoder_states)
decoder_states_ip = get_decoder_states()

decoder_outputs2, state_h2, state_c2 = get_decoder_lstm_layer(decoder_states_ip)

decoder_model = Model( [decoder_ip] + decoder_states_ip, [decoder_dense(decoder_outputs2)] + [state_h2, state_c2])

"""### Decoding Sentences"""

def get_zeros():
    dim = (1, 1)
    return np.zeros(dim)

def get_init_status(ip):
    return encoder_model.predict(ip)

def decode_seq(ip):

    decoded_sent = ''

    target_seq = get_zeros()
    itr = 0

    start_delimeter = tar_tkn_idx['START_']
    target_seq[0, 0] = start_delimeter

    states = get_init_status(ip)
    flag = 1

    while flag:
        
        temp = [target_seq] + states

        op_tkn, val1, val2 = decoder_model.predict(temp)

        otkn = op_tkn[0, -1, :]

        sampled_char = reverse_tar_idx[np.argmax(otkn)]

        decoded_sent = decoded_sent + reverse_tar_idx[np.argmax(otkn)]
        decoded_sent = decoded_sent + " "

        lsent = len(decoded_sent)

        if lsent > 50:
            flag = 0

        if reverse_tar_idx[np.argmax(otkn)] == '_END':
            flag = 0


        target_seq = get_zeros()
        
        target_seq[0, 0] = np.argmax(otkn)
        itr = 1

        states = [val1, val2]

    return decoded_sent

"""## BLEU Score"""

BATCH_SIZE = 1

def compute_score(s1, s2):
    return sentence_bleu(s1, s2)


def compute_BLUE_score(x, y):
    score = {}
    
    batch = create_batch(x, y)

    for sent in x:
        (ip, op), _ = next(batch)

        decoded_sent = decode_seq(ip)

        score[sent] = compute_score(sent.split(), decoded_sent.split())
    
    return score

train_BLUE = compute_BLUE_score(x_train, y_train)
test_BLUE = compute_BLUE_score(x_test, y_test)

def compute_avg_score(blue):

    total = 0
    cnt = 0

    for key in blue.keys():
        total += blue[key]
        cnt += 1

    return total/cnt

avg_bleu_train = compute_avg_score(train_BLUE)
print("Average Bleu Score For Training Data : ", avg_bleu_train)

avg_bleu_test = compute_avg_score(test_BLUE)
print("Average Bleu Score For Test Data : ", avg_bleu_test)

def save_score(path, scr):
    f = open(path, "a")
    f.write(str(scr))

save_score(TRAIN_BLEU_PATH, train_BLUE)
save_score(TEST_BLEU_PATH, test_BLUE)

print("Files saved successfully!")

def translate_sent():
    ip = input("Enter sentence : ")

    batch = create_batch(x_train, y_train)
    (ip, op), _ = next(batch)
    decoded_sent = decode_seq(ip)

    print("Translated Sentence : ", decoded_sent)


translate_sent()